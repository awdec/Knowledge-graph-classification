AbstractWe address the problem of building and evaluating a com-
putational system whose primary objective is creativity. We
illustrate seven characteristics of computational creativity in
the context of a system that autonomously composes West-
ern lyrical music. We conduct an external evaluation of the
system in which respondents rated the system with regard to
each characteristic as well as with regard to overall creativ-
ity. Average scores for overall creativity exceeded the ratings
for any single characteristic, suggesting that creativity may
be an emergent property and that unique research opportuni-
ties exist for building CC systems whose design attempts to
comprehend all known characteristics of creativity.Introduction
AI has been outperforming humans on certain types of nar-
row intelligence tasks for quite some time [e.g., (Silver et al.
2016)]; however, this is still not the case for other types of
tasks, including especially those requiring creativity, leading
researchers to characterize computational creativity (CC) as
“the final frontier” of AI (Colton and Wiggins 2012). Build-
ing systems that are computationally creative requires both
the explicit identification of and the intentional operational-
ization of necessary characteristics of creativity.State-of-the-art foundation models exhibit autonomy and
generativity, producing impressive results, most notably lan-
guage and image artifacts. As a result, one might be tempted
to claim such systems are creative as a side-effect of their
primary objective (e.g., next-token prediction). However,
while autonomy and generativity are certainly necessary
for creativity, they are not sufficient, particularly in highly
structured and specialized domains (Ventura 2016). No-
tably, current artificial neural-network-focused implementa-
tions (including highly-successful transformer and large lan-
guage models) have demonstrated weakness when highly-
structured or constrained generation is needed, e.g., in cer-
tain forms of poetry and music (Hadjeres and Nielsen 2018;
Glines 2022).While the concept of creativity is likely inherently con-
testable, multiple characteristics have been identified as nec-
essary for computational creativity (Colton et al. 2015; Ven-
tura 2017; Jordanous 2014) with many, if not all, ultimatelyCopyright © 2024, Association for the Advancement of Artificial
Intelligence (www.aaai.org). All rights reserved.derived from research on human creativity [cf. (Csikszent-
mihalyi 1997)]. In what follows, we examine a set of seven
such characteristics of creativity: generation, knowledge
representation, intentionality, aesthetic, domain knowledge,
autonomy and self-evaluation, and we do so in the context of
a highly-structured domain of creativity, namely structured
lyrical lead sheet generation for Western-popular music.We present an autonomous system for music composition
explicitly designed to be creative. The system incorporates
a variety of machine learning models aimed at achieving
(many of) the characteristics that distinguish creativity, and
other work details these models and how they address as-
pects of the problem of creativity (in the context of pop mu-
sic composition). Here, we present the complete system for
the first time. We evaluate the system to assess the extent to
which observers perceive these characteristics individually
compared to their perception of creativity overall. Results
show that the perception of creativity overall generally ex-
ceeds the perception of any of its constituent characteristics
individually, emphasizing that creativity emerges from, and
is greater than, the sum of its parts.Characteristics of Creative SystemsA high-level overview of the system—named Pop* (Pop
Star) —is shown in Figure 1. Pop* searches Twitter for a
posting that appeals to its aesthetic. This tweet serves as
an originating idea from which Pop* formulates its own
intention—a theme that the system will communicate in the
form of a novel music composition. Pop* then starts a learn-
ing phase in which lyric and sheet music databases are fil-
tered based on relevance to Pop*’s chosen intention. Pop*
uses these filtered training sets to train Markov models for
chords, rhythm, pitch, and lyrics. Besides learning models
for local structure, Pop* also learns global structural pat-
terns (e.g., musical motifs, verse-chorus structure, rhyme
schemes, etc.) from existing sheet music. The generative
process utilizes constrained Markov models to produce a
novel composition that is scored and filtered according to
an intention-driven self-evaluation function. Compositions
that pass the self-evaluation phase are rendered as audio and
sheet music.x = x1, . . . , xn to X = X1, . . . , Xn satisfies C1 if ∀i, xi
satisfies ci. PN (x) ∝ PM (x) if x satisfies C1 and is 0 oth-
erwise. For details see (Pachet, Roy, and Barbieri 2011).Pop* builds a modified NHMM N = (n, M, C1, C2) that
allows the definition of a set C2 of binary constraints. These
binary constraints are essential to creating rhymes, motifs,
and sectional form structure. Such a model is trained for
each of the chord, pitch, rhythm, and lyric viewpoints, re-
sulting in the four models: Nc, Np, Nr, Nl. The details of
this implementation are presented in (Bodily and Ventura
2022).Characteristic #2: Knowledge Representation
A knowledge representation defines a structured model of
problems, the cognitive process for solving these problems,
and the artifacts themselves. There is often a distinction be-
tween internal and external representations, with internal
representation in some cases being conflated with the choice
of generative model (Ventura 2017). The internal representa-
tion determines the ability to generalize within or across do-
mains (Lake, Salakhutdinov, and Tenenbaum 2015; Ventura
2016). Some forms of knowledge representation are man-
ually crafted (e.g., rule-based systems or predefined gram-
mars) whereas others are learned from observing existing
domain artifacts (i.e., machine learning).The knowledge representation in Pop* uses a hierarchi-
cal Bayesian program learning (HBPL) model consisting
of a hand-crafted hierarchy of human-level concept mod-
els each trained from data (Lake, Salakhutdinov, and Tenen-
baum 2015). The composition of a lead sheet γ for a given
aesthetic A is modeled asP (γ|A) = P (ν|A) · P (τ )· P (η|ν, τ ) · P (ϕ|ν, τ, η) · P (ρ|ν, τ ) · P (λ|ν, τ, ρ),where ν, τ , η, ϕ, ρ, and λ represent intentions, structure,
chords, pitch, rhythm, and lyrics, respectively.Pop* is able to individually train each submodel on a
potentially different dataset: P (η|ν, τ ), P (ϕ|ν, τ, η), and
P (ρ|ν, τ ) on a knowledge base of music and P (λ|ν, τ, ρ)
on a knowledge base of lyrics. We use NHMMs to imple-
ment the models for sampling η, ϕ, ρ, and λ. Pop* is able
to generalize its learning in specific areas and generate de-
scriptions of design decisions at varying levels of detail. In
addition to sheet music and audio artifacts, Pop* outputs a
short description that explains its intention for the composi-
tion, the source of its intention, and a comment evaluating
how well the intention was accomplished.Characteristic #3: Intentionality
An intentional system is a deliberative system whose arti-
facts are the result of a directed process towards a particu-
lar objective (Ventura 2016; Ackerman et al. 2017). Inten-
tionality is reflected in a system’s ability to choose its own
objectives (Guckelsberger, Salge, and Colton 2017) and to
evaluate its own success in accomplishing goals (Ackerman
et al. 2017). Intentions can relate to content, style, external
impact, or type of generative act (Colton, Pease, and Charn-
ley 2011), or other facets of creativity. To effectively en-
hance the perception of creativity, intention must be specificFigure 1: Pop* process diagram. The system originates an
intention from social media posts that appeal to its aesthetic,
which guides the training of generative models through a tar-
geted selection of lyrics and lead sheets. Generated artifacts
are output if they pass an intention-driven self-evaluation.Characteristic #1: Generation
A generative system produces artifacts that can be argued
to be more or less creative in the context of a broader, cul-
turally defined class (Wiggins 2006). In addition to tradi-
tional conceptualizations of creative artifacts, the process of
generation can also include more abstract elements includ-
ing descriptions of production methods, process, and means
of evaluation (Ritchie 2007; Colton 2008; Colton, Pease,
and Charnley 2011). Generative systems can range from the
purely stochastic to creation with the aid of perceptual facul-
ties (Ventura 2016), raising questions as to whether knowl-
edge of the system’s process is needed to determine its cre-
ativity (Boden 2004; Kasof 1995; Ventura 2017).To achieve local cohesive modeling without loss of global
structure, Pop* uses constrained or non-homogeneous
Markov models (NHMMs) (Bodily and Ventura 2022).
These models—built from a Markov model and a set of
constraints—were first designed to model small-scale tran-
sition patterns while allowing for constraints to be imposed
at various positions. These constraints can be used to create
the impression of global structure (Barbieri et al. 2012).A NHMM N = (n, M, C1) models the probability of a
sequence of random variables X = X1, . . . , Xn. Defining
N requires defining a sequence length n, a Markov model
M , and a set of unary constraints C1 = {c1, . . . , cn}. Each
unary constraint ci ∈ C1 represents a function fi(x) that
maps assignments of Xi (i.e., x ∈ Σ) to either 1 (ci is
satisfied) or 0 (ci is not satisfied). A particular assignmentjoy
sadnessoptimism
disappointment
positive emotion
negative emotionfear
timidityvector Em(t) = ((v1, w1), . . . , (v200, w200)) asemotion score(t) =Xiwi s.t. vi ∈ ETable 1: Emotion Topics. Empath topics used to find emo-
tionally charged tweets.enough to pose a challenge without being so focused as to
suggest determinism. (cf. the balance required for inducing
flow states as described by (Csikszentmihalyi 1997).)Pop* explicitly models intention as a vector of topics or
emotions V = ((v1, w1), . . . , (vn, wn)) where (vi, wi) rep-
resents a topic and its weight. The system’s objective is to
create music that communicates each topic vi to an extent
commensurate with its weight wi.Pop* computes an intention V using Stanford’s Empath
library (Fast, Chen, and Bernstein 2016). Given a text in-
put ι, Empath creates Em(ι) = ((v1, w1), . . . , (v200, w200))
where vi represents one of a set of 200 topics (defined by
Empath) and wi ∈ [0, 1] represents the strength of the se-
mantic relationship between ι and vi. The weights are nor-
malized such that P
i wi = 1. To establish an intention,
Pop* assigns V = Em(˜t) where ˜t is some text (e.g., a
tweet). The weights of all but the two highest scoring topics
in V are set to 0.0 to narrow and focus the system’s intention
on the most important topics (in the case of ties, scores for
all topics with tying scores are kept).Characteristic #4: AestheticIn contrast to intention, aesthetic plays a more persistent
role in determining style and is represented by the ability to
have opinions, attitudes, or beliefs about what is beautiful,
good, and creative (Papadopoulos and Wiggins 1999; Ko-
ren 2010; Mothersill 2004; Wiggins 2006). Simple systems
may rely on manual encoding of an aesthetic (Ventura 2017);
however, more creative systems may autonomously initi-
ate and make changes to their aesthetic standards (Colton,
Pease, and Charnley 2011; Jennings 2010). A system may
form aesthetic standards with respect to skill, imagination,
and appreciation (Colton 2008); value and surprise (Boden
2004); complexity (Hofstadter 1980); order (Birkhoff 1933);
and entropy (Shannon 2001). An ideal aesthetic should be
explainable (Bodily and Ventura 2018).For Pop*, an aesthetic A = (θ0, . . . , θm) is a list of in-
terests where θi represents any keyword or phrase in which
Pop* is “interested”. In this formulation, Pop* has a favor-
able opinion of any interest listed in A and no opinion about
anything else. For the results below, A = (“being in love”,
“feeling depressed”, “new beginnings”).For each theme θi ∈ A, Pop* searches Twitter for (up to)
500 tweets from the prior 24 hours using θi as the search
key to obtain a set of tweets T . From T , Pop* filters out
retweets and tweets of less than 100 characters. For each
remaining tweet t ∈ T , the Empath vector Em(t) is com-
puted. An emotion score(t) is computed using the Empathwhere E is the subset of Empath topics representing emo-
tions (see Table 1). Pop* retains the set Tθi of ten tweets
with the highest emotion score values for each interest θi.
i Tθi one tweet ˜t is selected at random as an origi-
nating tweet for the system’s generative process.Characteristic #5: Domain Knowledge
Domain knowledge includes an understanding of the struc-
ture of artifacts within the domain, requirements for inclu-
sion within the domain, and principles governing the evalu-
ation of artifacts within the domain. Domain knowledge is
often based on an inspiring set of domain-representative ar-
tifacts, which serves as inspiration during the generative pro-
cess, enabling the system to evaluate novelty and typicality
as a function of similarity to existing items (Ritchie 2007);
learn from examples that are devoid of the designer’s (ex-
plicit) biases or “fingerprints” (Ventura 2016); and socially
evolve through addition and subtraction of information, in-
cluding adding its own (successful) artifacts to its knowl-
edge base (P´erez y P´erez and Sharples 2004).Pop* leverages domain knowledge through machine
learning on two primary knowledge bases. The lyric knowl-
edge base (LKB) comprises lyrics from 369,606 songs
scraped from www.lyrics.com. Lyrics L for each song are
annotated with an Empath vector Em(L). Given an inten-
tion ν, Pop* selects a subset of the LKB for training deter-
mined by computing the Euclidean distance ∥ν − Em(L)∥
for each song L ∈ LKB. The closest k songs are selected,
with k varying as a function of the length of the song to be
generated (generally 3000-4000 songs). The music knowl-
edge base (MKB) comprises 6,673 pop music lead sheets
from the Wikifonia dataset. Each contains chords, melody,
and lyrics in Music XML format. Lyrics L for each song
are annotated with an Empath vector Em(L). Pop* selects
a subset of the MKB (generally 75-150 songs) for training
using the same Euclidean distance method as LKB subse-
lection. The MKB is used to train chord, pitch, and rhythm
models. For details on the preparation, parsing, and training
from these knowledge bases, see (Bodily and Ventura 2022).
Pop* also possesses perceptual faculties for detecting mu-
sical structure in symbolic music (i.e., Music XML files)
modeled on the way humans detect structure when listen-
ing to music through a process of mentally aligning musi-
cal subsequences that share similar chord, pitch, rhythm, or
lyric features. Structural alignments in a single viewpoint are
called motifs. Structural alignments across multiple view-
points create sectional form (e.g., a chorus results from over-
lapping repeats of lyrics, pitch, rhythm, and chords).Alignment is performed using a multi-Smith-Waterman
(mSW) self-alignment algorithm which aligns a song against
itself for each viewpoint and finds all unique local align-
ments with scores above some threshold. The scoring func-
tion used to calculate alignments considers different features
for each viewpoint. Weights for the scoring function (as wellas the threshold) are learned a priori using a genetic algo-
rithm trained on a small, structurally annotated subset of the
MKB. See (Bodily and Ventura 2021) for details.For each new composition, Pop* selects at random a
song in the MKB after which to structurally model the new
composition. The song’s length determines the length n of
the new composition needed to instantiate NHMMs Nc,
Np, and Nr for approximating P (η|ν, τ ), P (ϕ|ν, τ, η), and
P (ρ|ν, τ ). Pop* performs an mSW-alignment on this train-
ing song for each viewpoint. Each viewpoint-specific align-
ment defines a set of binary constraints C2 that is used to
define NHMMs Nc, Np, Nr, Nl.Characteristic #6: Autonomy
Autonomy is commonly cited as one of the most essen-
tial yet difficult characteristics to achieve in computational
creative systems (Mumford and Ventura 2015). Autonomy
is not a binary characteristic in creative systems but rather
manifests itself along a spectrum, with the most creative
systems exhibiting autonomy through self-evaluation, the
injection of knowledge “without leaving the injector’s fin-
gerprints,” and (at the acme) the ability to use perceptual
abilities to self-improve the system (Ventura 2016). In short,
the more ways in which a system can exert autonomy, the
greater its ascribed creativity (Colton, Pease, and Charn-
ley 2011). Jennings defines three criteria required for a sys-
tem to be autonomous: autonomous evaluation, autonomous
change, and non-randomness (Jennings 2010). The first two
refer to an ability to independently decide how well a cre-
ation appeals to standards and to independently initiate and
guide changes to these standards. The third does not mean a
system cannot employ randomness, but rather that decisions
should generally reflect that the system operates (indepen-
dently) on a basis of persistent standards.Pop* implements autonomy in three principal ways: in
choosing an intention, in choosing a relevant knowledge
(sub)base, and in self-evaluation. Pop* derives an intention
ν from a tweet ˜t where the probability of choosing ˜t varies
according to its relation to the system’s aesthetic A. Besides
having dictated A, humans play no role in the selection of
˜t. Pop* self-selects a training set from its knowledge base
that relates to its intention ν. In the self-evaluation process,
the decision of whether or not to keep a song is determined
by how well the composition semantically achieves its in-
tention ν. Note that of the three, the latter two rely on the
system’s intention, underscoring the important role that in-
tention plays generally in enabling autonomy.Characteristic #7: Self-Evaluation
Self-evaluation constitutes more than the ability to reflect
on the novelty, typicality, and aesthetic value of its output;
it also includes evaluation of how well the output achieves
the system’s goals (Ventura 2016). Self-evaluation occurs
without consulting outside opinions and thus presupposes
autonomy (Jennings 2010). (Seeking outside opinions is an
important guide by which to initiate and change evaluative
standards, but a system should maintain and apply a stan-
dard in self-evaluation that is distinct from those of other
creative agents (Ackerman et al. 2017).) Self-evaluationcan occur post hoc (e.g., in systems that filter) or via a
“baked-in” approach where the system’s notion of what is
good is inherent in generative subprocesses. This can go so
far as allowing subprocesses to be aware of and influence
one another during generation (Linkola et al. 2017). Over
time, self-evaluation can fine-tune the generative process ei-
ther through direct modification of the generative model or
through the addition of generated artifacts to the system’s
knowledge base (P´erez y P´erez and Sharples 2004).For a given intention vector ν Pop* is given 6 hours to
compose up to 10 candidate compositions using the same
training sets (i.e., Markov models M ) but each with a poten-
tially unique structure (i.e., lengths n and binary constraint
sets C2). Pop* applies a scoring function S(γ) to a compo-
sition γ = (ν, c, p, r, l) with intention ν, chords c, pitches p,
rhythms r and lyrics l:S(γ) = δ + σ + E + Rwhere δ = ∥ν − Em(l)∥; σ = |uniq(l)|/|l| (i.e., the
ratio of the unique word count to the total word count);
E = P|p|−31
|uniq((pi, . . . , pi+31))| (i.e., the average num-
ber of unique pitch values in p in a 4-measure sliding win-
dow); and R = |{pi|pi ∈ MIDI [60, 76]}|/|p| (i.e., the frac-
tion of pi ∈ p for which the MIDI value of pi is in the range
[60,76]). δ assesses how well the lyrics l reflect the sys-
tem’s intention ν. σ protects against overly repetitive lyrics.
As a way of measuring “catchiness” or managing boredom,
E represents a collective density value (Eigenfeldt et al.
2017)). R represents the sing-ability of the melodic pitch
sequence. The candidate γ with the highest self-evaluation
score S(γ) is output.External Evaluation of Creative
Characteristics
Ritchie suggests that a system’s creativity may (and possibly
should) be assessed through the external artifact—without
any knowledge of the system’s process (Ritchie 2007). In
contrast, others suggest that consideration of the system’s
process is essential (Kasof 1995; Boden 2004; Colton 2008;
Ventura 2016; Colton, Pease, and Charnley 2011). We evalu-
ate Pop* in both modalities, using Jordanous’ SPECS frame-
work (Jordanous 2012) which requires stating one’s defini-
tion of what it means to be creative (as done above) and then
creating and implementing assessments to measure to what
extent that creativity has been achieved.We conducted an evaluation in the form of an online
Qualtrics survey of 125 people. In each survey, the system
presents itself: “Hi, my name is Pop*! I am a computer sys-
tem that composes pop, rock, and show tune music. I read a
lot on Twitter. When I find a tweet that I like, then I compose
music.” The survey then proceeds in two stages: an eval-
uation based solely on external artifacts and an evaluation
based on an informed understanding of Pop*’s process.Evaluation Based on Artifacts The survey first invites
the participant to listen to and evaluate two original Pop*
compositions. Compositions for evaluation were chosen
completely at random from Pop*’s output. Twelve unique
compositions were included in the evaluation. These songsinclude significant variation in chords, melody,
length, structure, tempo, modality, key, and intention.lyrics,For each composition, the system’s generated description
of the piece is presented along with an audio recording of
the song. After reading the description and listening to the
audio, the participant is asked seven Likert scale questions:
1. How would you rate this composition overall?
2. How would you rate the lyric composition in this piece?
3. How would you rate the music composition in thispiece?4. How would you rate the global structure (i.e., form, lay-out) in this piece?5. How typical is this song of pop/rock/show tunes music?
6. How novel is this song compared to other pop/rock/showtunes music?7. How well did Pop* communicate its intention (X andY ) through the music?Each question is rated on a scale from 1 to 5, the first four
as star ratings with no labels (half stars allowed) and the
remaining three as Likert scales which label only the lowest
and highest ratings in defining the spectrum (Likert 1932).Evaluation Based on Process Upon completion of the
artifact-based evaluation, the participant is then introduced
to the process of Pop*. This introduction consists of a brief
description accompanied by a simplified version of Figure 1.
The simplified version uses the labels “Tweets”, “Interests”,
“Intention”, “Lyrics”, “Sheet Music”, “Generation”, “Eval-
uation”, and “Output” in place of the pertinent labels in Fig-
ure 1 to avoid overtly biasing answers. The participant is
then given 5 Likert scale questions:
8. How convinced are you that Pop* internally representsknowledge of music?9. How convinced are you that Pop* has an opinion, belief,or attitude about what makes music “good”?10. How much autonomy would you say Pop* has to makedecisions on its own?11. How good is Pop* at self-evaluation (i.e., judging itsown success)?12. How would you rate the creativity of Pop*We also invited the participant to explain via free response
their answer to question 12, to add other comments, and to
provide some demographic information to control for biases.
Of 125 respondents, 68 (54.4%) considered themselves mu-
sicians; 77 (61.6%) reported knowing the system designers
personally; 53 (42.4%) believed “Absolutely” that comput-
ers are capable of creativity, 6 (4.8%) believe they “Never”
will be, and 66 (52.8%) were “Not sure”.Results
Results of the survey are found in Tables 2-4. The “Best
Song” reported in Table 2 refers to the song with the high-
est average ratings for question Q1 (overall rating) across all
participants1. All scores are out of 5 with 1 representing the
minimum score possible.1And I think I took 10th place in the 2022 AI Song ContestFamiliarity Bias Fundamental to the notion of achieving
creativity in computational systems is that they are deemed
to be creative by unbiased observers. Although there is of-
ten bias against computers being creative, familiarity may
introduce a bias towards ascribing creativity.We find that there is a noticeable drop in scores between
the group of survey participants who responded “Yes” and
those that responded “No” to knowing one of the system
designers personally, both in the overall average scores for
each question, but also in the highest song-specific averages
(see Table 3). There was not a noticeable skew in beliefs
about the ability of computer creativity among the group not
familiar with the system designers.Even given the familiarity bias, we see the same general
trends in which aspects of creativity participants are most
willing to ascribe to Pop*. Novelty scores (Q6) have the
highest average and max scores for both groups. Likewise
both groups were least impressed by the system’s lyric abil-
ities (Q2). Significant to our work on incorporating struc-
ture is the fact that both groups have elevated ratings for the
structure in Pop* compositions (Q4).Generation Clearly the system generates artifacts. The
question that remains is whether these artifacts are novel
(Q6). The average and max novelty scores for both musi-
cians and non-musicians were the highest scores for any of
the characteristics of creativity as evaluated solely based on
artifacts. One respondent (not familiar with the system de-
signers) responded that “Creativity is the ability to create
new things. Pop* can clearly create new things.”There were also some participants that did not feel Pop*
expressed novelty. One respondent wrote that the songs (s)he
heard “did not strike me as very original and creative. Sorry.”
This criticism may be in part a reflection of the two songs
that this participant was randomly assigned. Pop*’s per-
ceived novelty might be improved by constraining a distance
measure between output songs.Knowledge Representation Most groups felt that Pop*
had some internal representation of knowledge (Q8). Even
musicians—who by definition likely possess such a repre-
sentation themselves—rated Pop* (slightly) above average.
Those unfamiliar with the system designers rated knowledge
representation as slightly below average.Related to the notion of process, one respondent wrote
that “combining ideas and generating from them some-
thing completely new is the epitome of creativity, regardless
of whether or not the process was automated. Thus, even
though the machine is a machine it does mimic, in some
way at least, some form of creativity.”Intentionality Intentionality (Q7) received relatively low
scores, particularly from the group of respondents unfamil-
iar with the system designers, who rated the intentionality of
Pop* well below average .Several comments were made to explain these low inten-
tionality ratings. One respondent wrote: “Pop* [sic] seemed
to always start with something in mind but delivered some-
thing different.” We assume that this comment is referring
to the way that Pop* explicitly compares the topics of itsQ1Q2Q3Q4Q5Q6Q7Max Score
Best Song3.20/3.25
2.71/3.252.63/3.00
2.29/3.003.50/3.54
2.71/3.133.38/3.33
3.00/3.133.14/3.50
3.14/3.503.77/3.76
3.57/3.313.40/3.67
2.86/3.13Table 2: Evaluation Based on Artifact. Average ratings for each song: average musicians’ rating in italics followed by the
average of all ratings. “Best Song” is the song with the highest overall rating score (Q1) across all participants.Knows
usSurvey Avg
Song MaxDoesn’t
know usSurvey Avg
Song MaxQ12.93
3.20Q8Q9Q10 Q11 Q123.532.873.402.883.562.902.603.042.483.08Table 3: Familiarity Bias. Scores by participants’ familiarity with study designers. Averages are over all songs. Max is the
highest-rated song. The system was typically rated more creative by those familiar with the system designers.intention with the topics it perceives in its generated song
and how these often tend to be different topics. Based on
this assumption, this comment is valuable because it may
suggest one way that at least some audiences perceive inten-
tionality: assessing the success of the system in meeting its
goals is based, at least partially, on whether or not the sys-
tem believes it has met its goals. This is surely not univer-
sally the case; many observers likely autonomously assess
the system’s success in meeting its goals. However, finding
ways to emphasize aspects in which the system believes it
has achieved its intention may in some cases improve the
perception of intentionality.Some observers felt that the system had some intention-
ality: “good at finding ideas and creating sounds to match
those ideas and feelings.” This may result from the fact that
some songs exhibit intentionality better than others.Aesthetic The presence of aesthetic (Q9) is a quality Pop*
currently struggles to exhibit. This attribute scored third low-
est across the general survey population as well as across the
musical subpopulation. Among participants not biased by
familiarity, the question of whether or not Pop* has an opin-
ion or belief was also answered with ratings below average
(though not far below those of the general survey group).One of the challenges in creating a convincing model of
aesthetic is that unlike novelty, or even intention, aesthetic
gets at the heart of what many people see as the fundamental
difference between computers and humans. As one respon-
dent put it: “It’s a machine, it has no autonomy or beliefs.”
Conversely, others were willing to give Pop* some credit:
“there were some sentiments communicated” and “Pop*’s
music sounds . . . different enough to have its own style.”Domain Knowledge Typicality (Q5) was the only char-
acteristic for which the group unbiased by familiarity rated
Pop* higher than the group biased by familiarity. This could
be explained by the fact that this word can have a negative
connotation outside of CC, and thus the bias for this ques-
tion might be expected toward lower scores. Either way, we
consider elevated scores for this attribute to be a good sign,especially given the parallel trends in the novelty score.Also related to domain knowledge were questions Q2, Q3,
and Q4, which rated Pop*’s lyric model, music model, and
structure model respectively. Ratings for lyrics were uni-
versally the lowest ratings for any question, with musicians
being particularly critical. Although Pop* has focused rela-
tively little on semantic cohesiveness of lyrics, it has focused
a great deal on how lyrics should be effectively combined
with music, e.g., to emphasize appropriate stresses and to
effectuate rhyme. This subtlety was observed by one com-
menter that wrote: “To put together lyrics to match the music
. . . takes a lot of creativity.”Overall music scores (Q3) were, on the other hand, among
the highest for any of the questions, suggesting that the
chord, pitch, and rhythm models are learning and applying
knowledge effectively. Many made comments such as “the
generated chord structure of the two songs impressed me.”Structure scores (Q4) were generally below average, al-
though there were a few songs that earned average ratings
above 3.0. This is an area that would benefit from further
assessment about what kinds of impact the structure may or
may not be having (e.g., does structure make the song easier
to remember or to get stuck in one’s head?).One respondent remarked on the importance of leverag-
ing domain knowledge, explaining that their attribution of
creativity to Pop* was based on its perceived ability “to cre-
ate songs based off of lots of different information and still
make it sound appealing and applicable.”Autonomy Autonomy (Q10) scored above average in
nearly every group, including those not biased by familiar-
ity with the system’s designers (the one exception was those
with the belief that computer creativity is impossible). Au-
tonomy was rarely the highest scoring characteristic, and
several of the comments against the creativity of the sys-
tem were aimed at the absence of autonomy. For example,
“Pop*’ [sic] follows the formulas really well, but the ideas
are not innovative, they are formulaic”, or, “Pop [sic] follows
a sequence of preprogrammed algorithms. I would thereforeMusicianship
All Groups Musicians Non-Musicians BelieversBelief about computer creativitySkeptics UnbelieversQ8
3.33Table 4: Evaluation Based on Process. Average scores for survey questions Q8–Q12, broken down by demographic.describe the output of Pop* as being more representative of
the creativity of its designer as well as the thoughts/moods
of the people giving input tweets.”Mumford and Ventura reported similar responses in their
work to assess the autonomy of CC systems (Mumford and
Ventura 2015). However, in their work, what respondents
thought was a computer being creative was actually a hu-
man being creative. There exists a preconceived notion that
regardless of what occurs, “It’s a machine, it has no auton-
omy.” To this point Mumford and Ventura suggest that “even
creative humans could be argued to be following a complex
set of chemical and psychological instructions” (Mumford
and Ventura 2015). An interesting future study might exam-
ine how changing people’s perception about their own auton-
omy would impact their perception of computer autonomy.Self-Evaluation The collective survey group rated the
self-evaluative abilities of Pop* (based on an understanding
of its process) lower than any other property of the system
except its lyric generation (Q11). This may result from the
general perception that Pop* consistently composes below-
average music (per responses to Q1). It may be hard to as-
cribe self-evaluative abilities to a system that can’t generate
cohesive English. It may also stem from the way that “Pop*
seemed to always start with something in mind but delivered
something different”.Q11 was included as an assessment based on Pop*’s pro-
cess under the assumption that a knowledge of the process
is critical to assessing whether the system uses a reasonable
self-evaluation method. However, it may be that the question
of efficacy in self-evaluation is better suited as an artifact-
based evaluation, if attributing success in self-evaluation de-
pends on the reviewer agreeing with the individual evalu-
ations that the system makes rather than agreeing with the
process it uses to make those evaluations.Like creativity, each characteristic attribute occurs on a
spectrum. The results of the evaluation point out opportuni-
ties for improvement; however, the results also suggest that,
at least to some extent, Pop* possesses each of the charac-
teristics necessary for creativity.Creativity Assessing any one of the above characteristics
of creativity may prove to be just as difficult as directly ask-
ing the question of interest: “Is the system creative?” In ask-
ing this question, we were careful to explicitly differentiate
the creativity of the system and that of its designers. Re-
sponses to this question (Q12) in the survey reported the
highest scores across all groups: those familiar with the de-
signers, those not familiar, musicians, non-musicians, be-lievers, skeptics, and unbelievers (novelty score for those
familiar with the designers, which was higher than that of
the group’s creativity score, is the only exception). Scores
across all groups were above average.The high ratings for the direct question of Pop*’s “cre-
ativity” is quite remarkable. It may suggest that, despite a
relative lack of the other characteristics examined, creativ-
ity can exist. Perhaps it might suggest the wrong criteria
have been chosen, and that if we had chosen the right criteria
then the scores for those criteria would have been a better re-
flection of the scores for creativity. However, there seems a
more probable explanation, which is that creativity (Q12) is
greater than the sum of its individual characteristics. In other
words, the perception of creativity in computational systems
can exceed the perception of individual creative characteris-
tics when these characteristics are found together.DiscussionStill in its early stages, CC has largely been defined by sys-
tems that exhibit one or a few creative characteristics. This
is understandable considering that endowing or augmenting
a system with any one of these characteristics is a non-trivial
task and that there are cases where it might be desirable to
focus exploration on a single characteristic. It is important,
however, that we embrace the challenge and keep an eye on
the goal of holistic computational creativity—the idea that
creativity most effectively emerges from the confluence of
the set of creative attributes. A system that possesses any one
of the characteristics of creativity is just that: a system that
possesses one of the characteristics of creativity. For such
systems, it is easy for critics to focus on how the system is
not creative. Conversely, by simply possessing some level of
many creative attributes a system may reasonably be seen as
creative, particularly among non-specialists.We have identified seven common, fundamental charac-
teristics that we believe to be necessary (though perhaps
not sufficient) for creativity. We have demonstrated the joint
application of these characteristics in an applied example
of a music composition system. We have externally eval-
uated this system for these characteristics through the use
of a combination of artifact-based and process-based assess-
ments. Our findings from these assessments suggest that the
system does possess the characteristics of creativity to vary-
ing extents, but that more importantly, the system overall is
perceived to be creative. This suggests that holistic CC rep-
resents a promising direction for future work in our efforts
to expand the “final frontier” of artificial intelligence.